{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Supabase-projektin alustus ja tietokantarakenteen luominen",
        "description": "Luo Supabase-projekti ja määritä tarvittavat tietokantataulut sisältöjen, käyttäjäprofiilien ja interaktioiden tallentamiseen.",
        "details": "1. Rekisteröidy Supabaseen (https://supabase.com) ja luo uusi projekti\n2. Aktivoi pgvector-laajennus tietokannassa (Database → Extensions → vector)\n3. Luo seuraavat tietokantataulut:\n   - **content**: id (PK), created_at, source (text), url (text), title (text), full_text (text), metadata (JSONB), embedding (vector)\n   - **profiles**: id (viittaa auth.users), updated_at, interest_vector (vector), interests (JSONB)\n   - **interactions**: id, user_id, content_id, interaction_type (text, esim. 'like', 'save', 'hide'), created_at\n4. Määritä tarvittavat indeksit suorituskyvyn optimoimiseksi\n5. Aseta Row Level Security (RLS) -säännöt tietoturvaa varten\n6. Luo API-avaimet sovelluksen käyttöön",
        "testStrategy": "1. Varmista, että tietokantataulut on luotu oikein tarkistamalla niiden rakenne Supabase-hallintapaneelista\n2. Testaa taulujen toimivuus lisäämällä testidataa SQL-editorin kautta\n3. Varmista, että pgvector-laajennus on aktivoitu ja toimii testaamalla vektorihakua\n4. Testaa RLS-sääntöjen toimivuus eri käyttäjärooleilla",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Supabase-projektin luominen",
            "description": "Rekisteröidy Supabaseen ja luo uusi projekti alustaa varten",
            "dependencies": [],
            "details": "1. Mene osoitteeseen https://supabase.com\n2. Rekisteröidy palveluun tai kirjaudu sisään\n3. Luo uusi projekti valitsemalla 'New project'\n4. Anna projektille nimi ja valitse tietokantasalasana\n5. Valitse projektin sijainti (region)\n6. Odota projektin luomisen valmistumista",
            "status": "done",
            "testStrategy": "Varmista, että pääset kirjautumaan uuteen projektiin Supabasen hallintapaneelissa"
          },
          {
            "id": 2,
            "title": "pgvector-laajennuksen aktivointi",
            "description": "Aktivoi pgvector-laajennus Supabase-tietokannassa vektorien käsittelyä varten",
            "dependencies": [
              1
            ],
            "details": "1. Siirry Supabase-projektin hallintapaneeliin\n2. Valitse 'Database' valikosta\n3. Mene 'Extensions' välilehdelle\n4. Etsi 'vector' laajennus listasta\n5. Aktivoi laajennus toggle-painikkeesta",
            "status": "done",
            "testStrategy": "Tarkista, että 'vector' laajennus näkyy aktiivisena Extensions-listassa"
          },
          {
            "id": 3,
            "title": "Tietokantataulujen luominen",
            "description": "Luo tarvittavat tietokantataulut sisältöjä, käyttäjäprofiileja ja interaktioita varten",
            "dependencies": [
              2
            ],
            "details": "1. Siirry 'Table Editor' -näkymään\n2. Luo 'content' taulu määritellyillä kentillä\n3. Luo 'profiles' taulu määritellyillä kentillä\n4. Luo 'interactions' taulu määritellyillä kentillä\n5. Varmista, että kaikki kentät ovat oikean tyyppisiä, erityisesti vector-tyyppiset kentät",
            "status": "done",
            "testStrategy": "Suorita SQL-kyselyt tarkistaaksesi, että taulut on luotu oikein määritellyillä kentillä"
          },
          {
            "id": 4,
            "title": "Indeksien ja RLS-sääntöjen määrittäminen",
            "description": "Aseta tarvittavat indeksit suorituskyvyn optimoimiseksi ja määritä Row Level Security -säännöt",
            "dependencies": [
              3
            ],
            "details": "1. Luo indeksit usein käytetyille hakukentille kuten 'content.title' ja 'interactions.user_id'\n2. Aseta RLS-säännöt 'content', 'profiles' ja 'interactions' tauluille\n3. Määritä politiikat, jotka rajoittavat pääsyä vain oikeutetuille käyttäjille",
            "status": "done",
            "testStrategy": "Testaa indeksien toimivuus EXPLAIN ANALYZE -komennoilla ja varmista RLS-sääntöjen toimivuus eri käyttäjärooleilla"
          },
          {
            "id": 5,
            "title": "API-avainten luominen",
            "description": "Luo tarvittavat API-avaimet sovelluksen käyttöön",
            "dependencies": [
              4
            ],
            "details": "1. Siirry Supabase-projektin asetuksiin\n2. Valitse 'API' valikosta\n3. Luo uusi API-avain sovellusta varten\n4. Määritä avaimelle sopivat oikeudet\n5. Tallenna API-avain turvalliseen paikkaan",
            "status": "done",
            "testStrategy": "Testaa luodun API-avaimen toimivuus tekemällä yksinkertainen API-kutsu Supabasen dokumentaation mukaisesti"
          }
        ]
      },
      {
        "id": 2,
        "title": "n8n-automaatioalustan käyttöönotto",
        "description": "Asenna ja konfiguroi n8n-automaatioalusta sisällön keruuta ja prosessointia varten.",
        "details": "1. Valitse n8n:n asennustapa (pilvipalvelu tai Docker-asennus)\n   - Pilvipalvelu: Rekisteröidy n8n.cloud-palveluun\n   - Docker: `docker run -it --rm --name n8n -p 5678:5678 -v ~/.n8n:/home/node/.n8n n8nio/n8n`\n2. Konfiguroi n8n-ympäristö:\n   - Aseta ympäristömuuttujat (API-avaimet, Supabase-yhteydet)\n   - Määritä ajastukset ja suoritusympäristö\n3. Asenna tarvittavat n8n-nodet ja laajennukset\n4. Luo perustyönkulku testausta varten\n5. Määritä käyttäjät ja käyttöoikeudet\n6. Konfiguroi varmuuskopiointi",
        "testStrategy": "1. Varmista, että n8n-instanssi käynnistyy ja on saavutettavissa\n2. Testaa yhteys Supabaseen yksinkertaisella workflow'lla\n3. Varmista, että ajastetut tehtävät toimivat\n4. Tarkista, että ympäristömuuttujat ovat saatavilla workflow'ssa",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "n8n Environment Setup and DeepSeek Node Configuration",
            "description": "Install n8n platform and configure DeepSeek API integration for LLM processing tasks",
            "dependencies": [],
            "details": "1. Install n8n using Docker: `docker run -it --rm --name n8n -p 5678:5678 -v ~/.n8n:/home/node/.n8n n8nio/n8n` 2. Access n8n interface at localhost:5678 3. Configure DeepSeek API credentials in n8n settings 4. Install HTTP Request node for DeepSeek API calls 5. Create reusable DeepSeek API connection template 6. Set up environment variables for API keys and endpoints",
            "status": "done",
            "testStrategy": "Test DeepSeek API connection with a simple text completion request to verify authentication and response handling"
          },
          {
            "id": 2,
            "title": "Reddit Content Ingestion Workflow",
            "description": "Create n8n workflow to fetch content from specified Reddit subreddits using Reddit API",
            "dependencies": [
              1
            ],
            "details": "1. Configure Reddit API credentials in n8n 2. Create HTTP Request node for Reddit API calls 3. Set up workflow trigger (schedule or manual) 4. Implement Reddit API authentication (OAuth2) 5. Configure subreddit content fetching (posts, comments) 6. Add data filtering for relevant content types 7. Implement rate limiting and error handling 8. Store raw Reddit data in temporary workflow variables",
            "status": "done",
            "testStrategy": "Execute workflow manually to fetch sample posts from target subreddits and verify data structure and completeness"
          },
          {
            "id": 3,
            "title": "YouTube Content Ingestion Workflow",
            "description": "Develop n8n workflow to extract content from YouTube channels and videos using YouTube Data API",
            "dependencies": [
              1
            ],
            "details": "1. Set up YouTube Data API v3 credentials 2. Create HTTP Request nodes for YouTube API endpoints 3. Implement channel video listing functionality 4. Add video metadata extraction (title, description, transcript if available) 5. Configure content filtering based on upload date and relevance 6. Implement pagination handling for large result sets 7. Add error handling for API quota limits 8. Store YouTube data in workflow variables",
            "status": "done",
            "testStrategy": "Test with specific YouTube channels to verify video metadata extraction and transcript retrieval functionality"
          },
          {
            "id": 4,
            "title": "Content Preprocessing and Normalization",
            "description": "Build preprocessing pipeline to clean and normalize content from Reddit and YouTube sources",
            "dependencies": [
              2,
              3
            ],
            "details": "1. Create Function nodes for text cleaning and normalization 2. Implement HTML/Markdown stripping for Reddit content 3. Add text length validation and truncation 4. Create content deduplication logic 5. Implement language detection and filtering 6. Add content quality scoring based on engagement metrics 7. Normalize data structure across different sources 8. Prepare content chunks for DeepSeek processing",
            "status": "done",
            "testStrategy": "Process sample content batches and verify cleaned output meets DeepSeek input requirements and quality standards"
          },
          {
            "id": 5,
            "title": "DeepSeek Content Analysis and Summarization",
            "description": "Implement DeepSeek API integration for content summarization, sentiment analysis, and key insight extraction",
            "dependencies": [
              4
            ],
            "details": "1. Create DeepSeek API request templates for different analysis types 2. Implement content summarization workflow using DeepSeek 3. Add sentiment analysis and topic extraction 4. Create key insight and trend identification prompts 5. Implement batch processing for multiple content pieces 6. Add response validation and error handling 7. Configure output formatting for downstream processing 8. Implement retry logic for API failures",
            "status": "done",
            "testStrategy": "Process test content through DeepSeek pipeline and validate summary quality, sentiment accuracy, and insight relevance"
          },
          {
            "id": 6,
            "title": "Supabase Database Integration",
            "description": "Configure Supabase connection and implement data storage workflows for processed content and insights",
            "dependencies": [
              5
            ],
            "details": "1. Set up Supabase project and database schema 2. Configure Supabase API credentials in n8n 3. Create database tables for content, summaries, and insights 4. Implement data insertion workflows using HTTP Request nodes 5. Add data validation and constraint handling 6. Create update mechanisms for existing content 7. Implement data archiving and cleanup processes 8. Add database connection error handling and retries",
            "status": "done",
            "testStrategy": "Execute end-to-end data flow from content ingestion to Supabase storage and verify data integrity and schema compliance"
          },
          {
            "id": 7,
            "title": "RSS Feed Generation and Notification System",
            "description": "Create automated RSS feed generation and implement notification system for new insights",
            "dependencies": [
              6
            ],
            "details": "1. Query processed insights from Supabase 2. Generate RSS XML format using Function nodes 3. Create RSS feed endpoint accessible via HTTP 4. Implement feed caching and update mechanisms 5. Add email notification workflow for new insights 6. Configure notification triggers and frequency 7. Create notification templates and formatting 8. Implement subscriber management if needed",
            "status": "done",
            "testStrategy": "Generate test RSS feed and verify XML validity, content accuracy, and notification delivery functionality"
          },
          {
            "id": 8,
            "title": "End-to-End MVP Testing and Workflow Orchestration",
            "description": "Integrate all components into complete workflow and perform comprehensive MVP testing",
            "dependencies": [
              7
            ],
            "details": "1. Create master workflow connecting all components 2. Implement proper error handling and logging throughout 3. Set up scheduled execution for automated operation 4. Configure workflow monitoring and alerting 5. Perform end-to-end testing with real data sources 6. Validate complete data flow from ingestion to RSS output 7. Test error scenarios and recovery mechanisms 8. Document workflow configuration and operation procedures",
            "status": "done",
            "testStrategy": "Execute complete MVP workflow multiple times with different content sources and verify consistent, accurate output generation and system reliability"
          }
        ]
      },
      {
        "id": 3,
        "title": "SvelteKit-projektin alustus ja Supabase-integraatio",
        "description": "Luo SvelteKit-projekti käyttöliittymää varten ja integroi se Supabase-backendin kanssa.",
        "details": "1. Luo uusi SvelteKit-projekti: `npm create svelte@latest my-app`\n2. Valitse TypeScript ja SPA-moodi\n3. Asenna tarvittavat riippuvuudet:\n   ```bash\n   cd my-app\n   npm install @supabase/supabase-js\n   npm install -D tailwindcss postcss autoprefixer\n   npx tailwindcss init -p\n   ```\n4. Luo Supabase-client (lib/supabaseClient.js):\n   ```javascript\n   import { createClient } from '@supabase/supabase-js'\n   \n   const supabaseUrl = import.meta.env.VITE_SUPABASE_URL\n   const supabaseAnonKey = import.meta.env.VITE_SUPABASE_ANON_KEY\n   \n   export const supabase = createClient(supabaseUrl, supabaseAnonKey)\n   ```\n5. Määritä ympäristömuuttujat (.env):\n   ```\n   VITE_SUPABASE_URL=https://your-project.supabase.co\n   VITE_SUPABASE_ANON_KEY=your-anon-key\n   ```\n6. Konfiguroi TailwindCSS (tailwind.config.js ja app.css)\n7. Luo perusrakenne sovellukselle (layout, reititys, komponentit)",
        "testStrategy": "1. Varmista, että sovellus käynnistyy: `npm run dev`\n2. Testaa Supabase-yhteys hakemalla testidataa\n3. Varmista, että ympäristömuuttujat toimivat\n4. Tarkista, että TailwindCSS on käytössä",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Sisällön keruun workflow n8n:ssä",
        "description": "Luo n8n-workflow, joka hakee sisältöä Redditistä ja YouTubesta ja tallentaa sen Supabaseen.",
        "details": "1. Luo uusi workflow n8n:ssä\n2. Lisää Schedule-node, joka ajastaa workflown (esim. 1h välein)\n3. Lisää HTTP Request -node Reddit-sisällön hakemiseen:\n   - URL: `https://www.reddit.com/r/{subreddit}/new.json?limit=25`\n   - Määritä useita subreddittejä konfiguraatiossa\n4. Lisää HTTP Request -node YouTube-sisällön hakemiseen YouTube API:n kautta\n5. Lisää Function-node, joka suodattaa jo olemassa olevat sisällöt:\n   ```javascript\n   const existingUrls = items[0].json.existing_urls;\n   return items.filter(item => !existingUrls.includes(item.json.url));\n   ```\n6. Lisää Supabase-node, joka tarkistaa olemassa olevat URL:t tietokannasta\n7. Lisää HTTP Request -node, joka hakee täyden sisällön (Reddit-postauksen teksti, YouTube-transkripti)\n8. Lisää Function-node, joka muotoilee datan tallennusta varten\n9. Lisää Supabase-node, joka tallentaa sisällön tietokantaan\n10. Konfiguroi error handling ja uudelleenyritykset",
        "testStrategy": "1. Testaa workflowta manuaalisesti n8n:n käyttöliittymässä\n2. Varmista, että sisältö haetaan oikein eri lähteistä\n3. Tarkista, että duplikaattisisältöjä ei tallenneta\n4. Varmista, että täysi sisältö (teksti, transkripti) haetaan oikein\n5. Tarkista, että data tallentuu oikeassa muodossa Supabaseen",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "LLM-integraatio sisällön arviointiin",
        "description": "Integroi kielimalli (LLM) sisällön arviointia ja pisteytystä varten n8n-workflowssa.",
        "details": "1. Lisää n8n-workflowiin OpenAI-node tai Mistral AI -node\n2. Konfiguroi LLM-arviointi monivaiheiseksi:\n   a. Tiivistä pitkä sisältö (esim. YouTube-transkripti):\n      ```\n      Prompt: \"Tiivistä seuraava teksti ytimekkääksi yhteenvedoksi (max 200 sanaa): {{$json.full_text}}\"\n      ```\n   b. Arvioi sisällön arvo ja relevanssi:\n      ```\n      Prompt: \"Arvioi seuraavan sisällön arvo ja relevanssi käyttäjälle, jonka kiinnostuksen kohteet ovat: {{$json.user_interests}}. Sisältö: {{$json.summary || $json.full_text}}. Anna arvio asteikolla 0-100 ja lyhyet perustelut.\"\n      ```\n3. Lisää Function-node, joka jäsentää LLM:n vastauksen ja erottaa numeerisen arvion ja perustelut\n4. Lisää OpenAI Embeddings -node, joka luo vektoriupotuksen sisällöstä\n5. Lisää Supabase-node, joka päivittää sisällön tietokantaan arvioinnin ja upotuksen kanssa\n6. Optimoi promptit ja parametrit (lämpötila, max_tokens) parhaan tuloksen saamiseksi\n7. Lisää virheenkäsittely ja uudelleenyritykset API-rajoitusten varalta",
        "testStrategy": "1. Testaa LLM-arviointia eri tyyppisillä sisällöillä (lyhyt/pitkä, eri aiheet)\n2. Varmista, että tiivistys toimii oikein pitkille sisällöille\n3. Tarkista, että arviointi on johdonmukaista ja järkevää\n4. Testaa vektoriupotusten laatua samankaltaisuushauilla\n5. Mittaa API-kutsujen määrää ja optimoi kustannustehokkuutta",
        "priority": "high",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Käyttäjäprofiilin hallinta",
        "description": "Toteuta käyttäjäprofiilin luonti, päivitys ja hallinta Supabasessa ja SvelteKit-sovelluksessa.",
        "details": "1. Konfiguroi Supabase-autentikaatio (sähköposti/salasana, OAuth)\n2. Luo SvelteKit-komponentit rekisteröitymiseen ja kirjautumiseen\n3. Toteuta käyttäjäprofiilin luonti rekisteröitymisen yhteydessä:\n   ```javascript\n   const { data, error } = await supabase.auth.signUp({\n     email: email,\n     password: password,\n   });\n   \n   if (data.user) {\n     await supabase.from('profiles').insert({\n       id: data.user.id,\n       interest_vector: null,\n       interests: initialInterests\n     });\n   }\n   ```\n4. Luo käyttöliittymä kiinnostuksen kohteiden määrittämiseen\n5. Toteuta käyttäjäprofiilin päivitys käyttäjän interaktioiden perusteella\n6. Luo Supabase Edge Function, joka päivittää käyttäjän interest_vectoria:\n   ```javascript\n   // supabase/functions/update-interest-vector/index.ts\n   import { serve } from 'https://deno.land/std@0.131.0/http/server.ts'\n   import { createClient } from 'https://esm.sh/@supabase/supabase-js@2.0.0'\n   \n   serve(async (req) => {\n     const { user_id, content_id, interaction_type } = await req.json()\n     \n     // Hae sisällön embedding ja käyttäjän nykyinen interest_vector\n     // Päivitä interest_vector painottaen sitä sisällön embeddingin suuntaan\n     \n     return new Response(JSON.stringify({ success: true }), {\n       headers: { 'Content-Type': 'application/json' },\n     })\n   })\n   ```\n7. Toteuta käyttäjäprofiilin hallintanäkymä, jossa käyttäjä voi muokata kiinnostuksen kohteitaan",
        "testStrategy": "1. Testaa rekisteröitymis- ja kirjautumisprosessi\n2. Varmista, että käyttäjäprofiili luodaan oikein\n3. Testaa kiinnostuksen kohteiden lisääminen ja poistaminen\n4. Varmista, että interest_vector päivittyy oikein interaktioiden perusteella\n5. Testaa profiilinhallintanäkymän toimivuus eri laitteilla",
        "priority": "medium",
        "dependencies": [
          1,
          3
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Sisältöjen ranking-algoritmi",
        "description": "Toteuta algoritmi, joka järjestää sisällöt käyttäjäprofiilin perusteella relevanssin mukaan.",
        "details": "1. Luo Supabase-funktio sisältöjen hakemiseen ja järjestämiseen:\n   ```sql\n   CREATE OR REPLACE FUNCTION get_ranked_content(user_id UUID)\n   RETURNS TABLE (id UUID, title TEXT, source TEXT, url TEXT, metadata JSONB, relevance_score FLOAT)\n   LANGUAGE plpgsql\n   AS $$\n   BEGIN\n     RETURN QUERY\n     SELECT \n       c.id, c.title, c.source, c.url, c.metadata,\n       1 - (p.interest_vector <=> c.embedding) as relevance_score\n     FROM content c, profiles p\n     WHERE p.id = user_id\n     AND p.interest_vector IS NOT NULL\n     AND NOT EXISTS (\n       SELECT 1 FROM interactions i \n       WHERE i.user_id = user_id AND i.content_id = c.id AND i.interaction_type = 'hide'\n     )\n     ORDER BY relevance_score DESC\n     LIMIT 50;\n   END;\n   $$;\n   ```\n2. Optimoi ranking-algoritmi huomioimaan:\n   - Sisällön tuoreus (painota uudempia sisältöjä)\n   - Käyttäjän aiemmat interaktiot (tykkäykset, tallennukset)\n   - Sisällön laatu (LLM-arviointi)\n3. Toteuta SvelteKit-komponentti, joka hakee ja näyttää järjestetyt sisällöt:\n   ```javascript\n   async function fetchRankedContent() {\n     const { data, error } = await supabase\n       .rpc('get_ranked_content', { user_id: currentUser.id })\n     \n     if (error) console.error('Error fetching content:', error)\n     else return data\n   }\n   ```\n4. Lisää välimuistitus suorituskyvyn parantamiseksi\n5. Toteuta inkrementaalinen lataus (infinite scroll)",
        "testStrategy": "1. Testaa ranking-algoritmin toimivuus eri käyttäjäprofiileilla\n2. Varmista, että järjestys on looginen ja relevantti\n3. Mittaa hakujen suorituskykyä ja optimoi tarvittaessa\n4. Testaa inkrementaalisen latauksen toimivuus\n5. Varmista, että piilotetut sisällöt eivät näy käyttäjälle",
        "priority": "high",
        "dependencies": [
          5,
          6
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Scrollattavan käyttöliittymän toteutus",
        "description": "Toteuta moderni, responsiivinen ja scrollattava käyttöliittymä sisältöjen esittämiseen.",
        "details": "1. Suunnittele sisältökortin komponentti (ContentCard.svelte):\n   ```svelte\n   <script>\n     export let content;\n     \n     async function handleLike() {\n       // Tallenna tykkäys ja päivitä käyttäjäprofiili\n     }\n     \n     async function handleSave() {\n       // Tallenna sisältö käyttäjän tallennettuihin\n     }\n     \n     async function handleHide() {\n       // Piilota sisältö käyttäjältä\n     }\n   </script>\n   \n   <div class=\"content-card\">\n     <h2>{content.title}</h2>\n     <p class=\"source\">{content.source}</p>\n     <div class=\"actions\">\n       <button on:click={handleLike}>Tykkää</button>\n       <button on:click={handleSave}>Tallenna</button>\n       <button on:click={handleHide}>Piilota</button>\n     </div>\n   </div>\n   ```\n2. Toteuta infinite scroll -toiminto:\n   ```javascript\n   let contents = [];\n   let page = 0;\n   let loading = false;\n   let hasMore = true;\n   \n   async function loadMore() {\n     if (loading || !hasMore) return;\n     \n     loading = true;\n     const newContents = await fetchContents(page);\n     loading = false;\n     \n     if (newContents.length === 0) {\n       hasMore = false;\n     } else {\n       contents = [...contents, ...newContents];\n       page++;\n     }\n   }\n   \n   function handleScroll() {\n     const bottom = window.innerHeight + window.scrollY >= document.body.offsetHeight - 500;\n     if (bottom) loadMore();\n   }\n   \n   onMount(() => {\n     loadMore();\n     window.addEventListener('scroll', handleScroll);\n     return () => window.removeEventListener('scroll', handleScroll);\n   });\n   ```\n3. Tyylittele käyttöliittymä TailwindCSS:llä\n4. Optimoi mobiilikokemus (touch-eleet, responsiivisuus)\n5. Lisää animaatiot ja siirtymät sujuvuuden parantamiseksi\n6. Toteuta sisällön esikatselu ja laajentaminen",
        "testStrategy": "1. Testaa käyttöliittymää eri laitteilla ja näyttöko'oilla\n2. Varmista, että infinite scroll toimii sujuvasti\n3. Testaa interaktioiden (tykkäys, tallennus, piilotus) toimivuus\n4. Varmista, että käyttöliittymä on esteetön ja käytettävä\n5. Mittaa suorituskykyä ja optimoi tarvittaessa",
        "priority": "medium",
        "dependencies": [
          3,
          7
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "YouTube-transkriptien hakutoiminto",
        "description": "Toteuta toiminto, joka hakee YouTube-videoiden transkriptit arviointia varten.",
        "details": "1. Lisää n8n-workflowiin YouTube-transkriptien hakutoiminto\n2. Käytä youtube-transcript-api:a tai vastaavaa kirjastoa:\n   ```javascript\n   // n8n Function node\n   const { YoutubeTranscript } = require('youtube-transcript');\n   \n   async function getTranscript(videoId) {\n     try {\n       const transcript = await YoutubeTranscript.fetchTranscript(videoId);\n       return transcript.map(item => item.text).join(' ');\n     } catch (error) {\n       console.error(`Failed to get transcript for ${videoId}:`, error);\n       return null;\n     }\n   }\n   \n   // Käsittele jokainen video\n   for (const item of items) {\n     if (item.json.source === 'youtube') {\n       const videoId = extractVideoId(item.json.url);\n       item.json.transcript = await getTranscript(videoId);\n     }\n   }\n   \n   return items;\n   ```\n3. Käsittele tapaukset, joissa transkriptiä ei ole saatavilla\n4. Optimoi transkriptien käsittely (esim. pitkien transkriptien tiivistäminen)\n5. Tallenna transkriptit Supabaseen osana sisällön metadata-kenttää\n6. Lisää virheenkäsittely ja uudelleenyritykset",
        "testStrategy": "1. Testaa transkriptien hakua eri YouTube-videoilla\n2. Varmista, että toiminto käsittelee oikein videot, joissa ei ole transkriptiä\n3. Testaa pitkien transkriptien käsittelyä\n4. Varmista, että transkriptit tallentuvat oikein tietokantaan\n5. Mittaa API-kutsujen määrää ja optimoi tarvittaessa",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Reddit-sisällön täysi lataus",
        "description": "Toteuta toiminto, joka hakee Reddit-postausten täyden tekstisisällön ja tarvittaessa kommentit.",
        "details": "1. Lisää n8n-workflowiin Reddit-sisällön täysi lataustoiminto\n2. Käytä Reddit JSON API:a täyden sisällön hakemiseen:\n   ```javascript\n   // n8n Function node\n   async function getFullRedditContent(postId) {\n     const response = await fetch(`https://www.reddit.com/comments/${postId}.json`);\n     const data = await response.json();\n     \n     const post = data[0].data.children[0].data;\n     const fullText = post.selftext || '';\n     \n     // Hae myös top-kommentit tarvittaessa\n     const comments = data[1].data.children\n       .filter(child => child.kind === 't1')\n       .slice(0, 5)\n       .map(child => child.data.body)\n       .join('\\n\\n');\n     \n     return {\n       title: post.title,\n       selftext: fullText,\n       top_comments: comments\n     };\n   }\n   \n   // Käsittele jokainen postaus\n   for (const item of items) {\n     if (item.json.source === 'reddit') {\n       const postId = extractPostId(item.json.url);\n       const fullContent = await getFullRedditContent(postId);\n       item.json.full_text = `${fullContent.title}\\n\\n${fullContent.selftext}`;\n       item.json.metadata = { ...item.json.metadata, top_comments: fullContent.top_comments };\n     }\n   }\n   \n   return items;\n   ```\n3. Käsittele eri postaus-tyypit (teksti, linkki, kuva, video)\n4. Optimoi sisällön käsittely ja tallentaminen\n5. Lisää virheenkäsittely ja uudelleenyritykset",
        "testStrategy": "1. Testaa sisällön hakua eri tyyppisistä Reddit-postauksista\n2. Varmista, että toiminto käsittelee oikein eri postaus-tyypit\n3. Testaa kommenttien hakua ja käsittelyä\n4. Varmista, että sisältö tallentuu oikein tietokantaan\n5. Mittaa API-kutsujen määrää ja optimoi tarvittaessa",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 11,
        "title": "Käyttäjän interaktioiden tallentaminen",
        "description": "Toteuta toiminnallisuus käyttäjän interaktioiden (tykkäys, tallennus, piilotus) tallentamiseen ja käsittelyyn.",
        "details": "1. Luo SvelteKit-komponentit interaktioiden käsittelyyn:\n   ```javascript\n   // src/lib/interactions.js\n   export async function saveInteraction(contentId, type) {\n     const { data: user } = await supabase.auth.getUser();\n     if (!user) return { error: 'Not authenticated' };\n     \n     const { data, error } = await supabase\n       .from('interactions')\n       .upsert({\n         user_id: user.id,\n         content_id: contentId,\n         interaction_type: type,\n         created_at: new Date().toISOString()\n       }, {\n         onConflict: 'user_id,content_id,interaction_type'\n       });\n     \n     if (!error && type !== 'hide') {\n       // Päivitä käyttäjäprofiili\n       await supabase.functions.invoke('update-interest-vector', {\n         body: { user_id: user.id, content_id: contentId, interaction_type: type }\n       });\n     }\n     \n     return { data, error };\n   }\n   ```\n2. Integroi interaktiot käyttöliittymään:\n   ```svelte\n   <!-- ContentCard.svelte -->\n   <script>\n     import { saveInteraction } from '$lib/interactions';\n     export let content;\n     \n     async function handleLike() {\n       await saveInteraction(content.id, 'like');\n     }\n     \n     async function handleSave() {\n       await saveInteraction(content.id, 'save');\n     }\n     \n     async function handleHide() {\n       await saveInteraction(content.id, 'hide');\n     }\n   </script>\n   ```\n3. Toteuta interaktioiden visualisointi (esim. tykkäys-nappi muuttuu aktiiviseksi)\n4. Lisää tallennettujen sisältöjen näkymä\n5. Toteuta toiminto interaktioiden peruuttamiseen (esim. tykkäyksen poistaminen)",
        "testStrategy": "1. Testaa interaktioiden tallentamista ja hakemista\n2. Varmista, että käyttäjäprofiili päivittyy oikein interaktioiden perusteella\n3. Testaa interaktioiden visualisointia käyttöliittymässä\n4. Varmista, että tallennettujen sisältöjen näkymä toimii oikein\n5. Testaa interaktioiden peruuttamista",
        "priority": "medium",
        "dependencies": [
          6,
          8
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Käyttäjäprofiilin päivitys interaktioiden perusteella",
        "description": "Toteuta mekanismi, joka päivittää käyttäjän kiinnostusprofiilia automaattisesti interaktioiden perusteella.",
        "details": "1. Luo Supabase Edge Function käyttäjäprofiilin päivittämiseen:\n   ```typescript\n   // supabase/functions/update-interest-vector/index.ts\n   import { serve } from 'https://deno.land/std@0.131.0/http/server.ts'\n   import { createClient } from 'https://esm.sh/@supabase/supabase-js@2.0.0'\n   \n   serve(async (req) => {\n     const { user_id, content_id, interaction_type } = await req.json()\n     \n     // Luo Supabase-client\n     const supabaseClient = createClient(\n       Deno.env.get('SUPABASE_URL') ?? '',\n       Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''\n     )\n     \n     // Hae sisällön embedding\n     const { data: content } = await supabaseClient\n       .from('content')\n       .select('embedding')\n       .eq('id', content_id)\n       .single()\n     \n     if (!content?.embedding) {\n       return new Response(JSON.stringify({ error: 'Content not found' }), {\n         headers: { 'Content-Type': 'application/json' },\n         status: 404\n       })\n     }\n     \n     // Hae käyttäjän nykyinen interest_vector\n     const { data: profile } = await supabaseClient\n       .from('profiles')\n       .select('interest_vector')\n       .eq('id', user_id)\n       .single()\n     \n     // Laske uusi interest_vector\n     let newVector\n     if (!profile?.interest_vector) {\n       // Jos käyttäjällä ei ole vielä interest_vectoria, käytä sisällön embeddingiä\n       newVector = content.embedding\n     } else {\n       // Muuten painota nykyistä vektoria sisällön embeddingin suuntaan\n       // Painotus riippuu interaktion tyypistä\n       const weight = interaction_type === 'like' ? 0.3 : \n                     interaction_type === 'save' ? 0.5 : 0.1\n       \n       newVector = profile.interest_vector.map((val, i) => {\n         return val * (1 - weight) + content.embedding[i] * weight\n       })\n     }\n     \n     // Päivitä käyttäjäprofiili\n     const { error } = await supabaseClient\n       .from('profiles')\n       .update({ interest_vector: newVector })\n       .eq('id', user_id)\n     \n     if (error) {\n       return new Response(JSON.stringify({ error: error.message }), {\n         headers: { 'Content-Type': 'application/json' },\n         status: 500\n       })\n     }\n     \n     return new Response(JSON.stringify({ success: true }), {\n       headers: { 'Content-Type': 'application/json' }\n     })\n   })\n   ```\n2. Optimoi vektorien päivityslogiikka (painotukset, normalisointi)\n3. Lisää aikaperusteinen painotus (uudemmat interaktiot vaikuttavat enemmän)\n4. Toteuta käyttäjäprofiilin visualisointi (esim. kiinnostuksen kohteiden näyttäminen)\n5. Lisää mahdollisuus manuaaliseen profiilin säätämiseen",
        "testStrategy": "1. Testaa profiilin päivitystä eri interaktiotyypeillä\n2. Varmista, että vektorien päivitys toimii matemaattisesti oikein\n3. Testaa profiilin päivitystä eri lähtötilanteista (uusi käyttäjä, olemassa oleva profiili)\n4. Varmista, että profiilin visualisointi vastaa todellista profiilia\n5. Testaa manuaalisen säädön vaikutusta sisältöjen rankingiin",
        "priority": "medium",
        "dependencies": [
          6,
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Ilmoitustoiminnallisuus uusista sisällöistä",
        "description": "Toteuta toiminto, joka ilmoittaa käyttäjälle uusista, erityisen merkityksellisistä sisällöistä.",
        "details": "1. Luo n8n-workflow, joka tunnistaa erityisen merkitykselliset sisällöt:\n   ```javascript\n   // n8n Function node\n   // Tunnista sisällöt, joiden relevanssi-score on korkea\n   const highRelevanceThreshold = 0.85;\n   \n   const highRelevanceContents = items.filter(item => \n     item.json.relevance_score > highRelevanceThreshold\n   );\n   \n   return highRelevanceContents;\n   ```\n2. Toteuta ilmoitusmekanismi:\n   - Selainilmoitukset (Web Push Notifications)\n   - Sähköposti-ilmoitukset\n   - Sovelluksen sisäiset ilmoitukset\n3. Luo SvelteKit-komponentti ilmoitusten näyttämiseen:\n   ```svelte\n   <!-- Notifications.svelte -->\n   <script>\n     import { onMount } from 'svelte';\n     import { supabase } from '$lib/supabaseClient';\n     \n     let notifications = [];\n     \n     onMount(async () => {\n       // Hae ilmoitukset\n       const { data } = await supabase\n         .from('notifications')\n         .select('*')\n         .order('created_at', { ascending: false })\n         .limit(10);\n       \n       notifications = data || [];\n       \n       // Tilaa reaaliaikaiset ilmoitukset\n       const subscription = supabase\n         .channel('notifications')\n         .on('INSERT', payload => {\n           notifications = [payload.new, ...notifications];\n         })\n         .subscribe();\n       \n       return () => subscription.unsubscribe();\n     });\n     \n     function markAsRead(id) {\n       // Merkitse ilmoitus luetuksi\n     }\n   </script>\n   \n   <div class=\"notifications-panel\">\n     {#each notifications as notification}\n       <div class=\"notification\" class:unread={!notification.read_at}>\n         <h3>{notification.title}</h3>\n         <p>{notification.message}</p>\n         <button on:click={() => markAsRead(notification.id)}>Merkitse luetuksi</button>\n       </div>\n     {/each}\n   </div>\n   ```\n4. Toteuta ilmoitusasetusten hallinta käyttäjälle\n5. Optimoi ilmoitusten ajoitus ja määrä käyttäjäkokemuksen parantamiseksi",
        "testStrategy": "1. Testaa merkityksellisten sisältöjen tunnistamista\n2. Varmista, että ilmoitukset toimivat eri alustoilla (selain, mobiili)\n3. Testaa ilmoitusten asetuksia ja käyttäjän preferenssien huomioimista\n4. Varmista, että ilmoitusten määrä on järkevä\n5. Testaa ilmoitusten merkitsemistä luetuksi",
        "priority": "low",
        "dependencies": [
          7,
          12
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Tallennettujen sisältöjen hallinta",
        "description": "Toteuta toiminnallisuus, jolla käyttäjä voi hallita tallentamiaan sisältöjä.",
        "details": "1. Luo SvelteKit-näkymä tallennettujen sisältöjen hallintaan:\n   ```svelte\n   <!-- src/routes/saved/+page.svelte -->\n   <script>\n     import { onMount } from 'svelte';\n     import { supabase } from '$lib/supabaseClient';\n     import ContentCard from '$lib/components/ContentCard.svelte';\n     \n     let savedContents = [];\n     let loading = true;\n     \n     onMount(async () => {\n       await fetchSavedContents();\n     });\n     \n     async function fetchSavedContents() {\n       loading = true;\n       \n       const { data: user } = await supabase.auth.getUser();\n       if (!user) return;\n       \n       const { data, error } = await supabase\n         .from('content')\n         .select('*')\n         .in('id', (\n           supabase\n             .from('interactions')\n             .select('content_id')\n             .eq('user_id', user.id)\n             .eq('interaction_type', 'save')\n         ));\n       \n       if (error) console.error('Error fetching saved contents:', error);\n       else savedContents = data || [];\n       \n       loading = false;\n     }\n     \n     async function removeFromSaved(contentId) {\n       const { data: user } = await supabase.auth.getUser();\n       if (!user) return;\n       \n       const { error } = await supabase\n         .from('interactions')\n         .delete()\n         .eq('user_id', user.id)\n         .eq('content_id', contentId)\n         .eq('interaction_type', 'save');\n       \n       if (!error) {\n         savedContents = savedContents.filter(content => content.id !== contentId);\n       }\n     }\n   </script>\n   \n   <div class=\"saved-contents\">\n     <h1>Tallennetut sisällöt</h1>\n     \n     {#if loading}\n       <p>Ladataan...</p>\n     {:else if savedContents.length === 0}\n       <p>Ei tallennettuja sisältöjä.</p>\n     {:else}\n       {#each savedContents as content}\n         <ContentCard {content}>\n           <button slot=\"actions\" on:click={() => removeFromSaved(content.id)}>Poista tallennetuista</button>\n         </ContentCard>\n       {/each}\n     {/if}\n   </div>\n   ```\n2. Lisää toiminnot sisältöjen järjestämiseen ja suodattamiseen\n3. Toteuta hakutoiminto tallennettujen sisältöjen etsimiseen\n4. Lisää mahdollisuus organisoida tallennettuja sisältöjä kategorioihin tai kokoelmiin\n5. Toteuta tallennettujen sisältöjen vienti ja tuonti",
        "testStrategy": "1. Testaa tallennettujen sisältöjen hakua ja näyttämistä\n2. Varmista, että sisältöjen poistaminen tallennetuista toimii\n3. Testaa järjestämis- ja suodatustoimintoja\n4. Varmista, että hakutoiminto löytää relevantit sisällöt\n5. Testaa kategorioiden ja kokoelmien toimivuutta",
        "priority": "low",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Sovelluksen käyttöönotto ja monitorointi",
        "description": "Valmistele sovellus tuotantokäyttöön, määritä monitorointi ja analytiikka.",
        "details": "1. Konfiguroi SvelteKit-sovelluksen tuotantoympäristö:\n   ```bash\n   npm run build\n   ```\n2. Määritä hosting-palvelu (esim. Vercel, Netlify, Cloudflare Pages):\n   - Luo deployment-konfiguraatio\n   - Määritä ympäristömuuttujat\n   - Konfiguroi CI/CD-pipeline\n3. Varmista Supabase-tuotantoympäristön asetukset:\n   - RLS-säännöt\n   - Indeksit\n   - Varmuuskopiointi\n4. Konfiguroi n8n-tuotantoympäristö:\n   - Ajastetut workflowt\n   - Virheenkäsittely ja ilmoitukset\n   - Monitorointi\n5. Lisää analytiikka ja monitorointi:\n   - Käyttäjäanalytiikka (esim. Plausible, Fathom)\n   - Virheenseuranta (esim. Sentry)\n   - Suorituskyvyn monitorointi\n6. Dokumentoi järjestelmä:\n   - Arkkitehtuuri\n   - API-rajapinnat\n   - Ylläpito-ohjeet",
        "testStrategy": "1. Testaa sovelluksen toimivuutta tuotantoympäristössä\n2. Varmista, että kaikki ympäristömuuttujat on määritetty oikein\n3. Testaa automaattisten workflowien toimivuutta\n4. Varmista, että analytiikka ja monitorointi toimivat\n5. Tarkista sovelluksen suorituskyky ja skaalautuvuus\n6. Testaa varmuuskopiointi ja palautus",
        "priority": "medium",
        "dependencies": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Investigate and Document Supabase pgvector Functionality",
        "description": "Research and document how to use Supabase's pgvector extension for storing embeddings, performing vector searches, and calculating cosine similarity.",
        "details": "1. Review Supabase pgvector documentation and implementation details:\n   - Study the pgvector extension documentation (https://github.com/pgvector/pgvector)\n   - Examine Supabase's specific implementation and API for vector operations\n\n2. Document embedding storage methods:\n   ```sql\n   -- Example of creating a table with vector column\n   CREATE TABLE items (\n     id bigint GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n     embedding vector(1536)\n   );\n   \n   -- Example of inserting vector data\n   INSERT INTO items (embedding) VALUES ('[0.1, 0.2, 0.3, ...]');\n   ```\n\n3. Document vector search methods:\n   - Nearest neighbor search using L2 distance (Euclidean)\n   ```sql\n   SELECT * FROM items ORDER BY embedding <-> '[query_vector]' LIMIT 5;\n   ```\n   \n   - Nearest neighbor search using cosine distance\n   ```sql\n   SELECT * FROM items ORDER BY embedding <=> '[query_vector]' LIMIT 5;\n   ```\n   \n   - Nearest neighbor search using inner product\n   ```sql\n   SELECT * FROM items ORDER BY embedding <#> '[query_vector]' LIMIT 5;\n   ```\n\n4. Document cosine similarity calculation:\n   ```sql\n   -- Cosine similarity = 1 - cosine distance\n   SELECT 1 - (embedding <=> '[query_vector]') as cosine_similarity FROM items;\n   ```\n\n5. Document indexing for performance optimization:\n   ```sql\n   -- Create HNSW index for faster vector searches\n   CREATE INDEX on items USING hnsw (embedding vector_cosine_ops);\n   \n   -- Create IVFFlat index (alternative)\n   CREATE INDEX on items USING ivfflat (embedding vector_l2_ops) WITH (lists = 100);\n   ```\n\n6. Create code examples for the project's context:\n   - Example for storing content embeddings\n   - Example for finding similar content\n   - Example for updating user interest vectors\n\n7. Document performance considerations:\n   - Index types and their trade-offs\n   - Query optimization techniques\n   - Scaling considerations for large vector datasets",
        "testStrategy": "1. Set up a test environment with Supabase and pgvector extension activated\n2. Create test tables with vector columns of different dimensions\n3. Insert sample embedding vectors into the tables\n4. Test vector search operations:\n   - Verify nearest neighbor search using different distance metrics\n   - Measure and compare performance with and without indexes\n   - Test with different vector dimensions and dataset sizes\n5. Test cosine similarity calculations:\n   - Compare results with manual calculations to verify accuracy\n   - Benchmark performance for different approaches\n6. Create and test practical examples relevant to the project:\n   - Store and retrieve content embeddings\n   - Find similar content based on embeddings\n   - Calculate similarity between user profiles and content\n7. Document all findings, including code examples, performance metrics, and best practices\n8. Create a comprehensive reference guide for the development team",
        "status": "pending",
        "dependencies": [
          1
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Implement and Test Row Level Security (RLS) for Profiles and Interactions Tables",
        "description": "Configure and test Row Level Security policies for the 'profiles' and 'interactions' tables to ensure users can only access and modify their own data.",
        "details": "1. Configure RLS policies for the 'profiles' table:\n   ```sql\n   -- Enable RLS on profiles table\n   ALTER TABLE profiles ENABLE ROW LEVEL SECURITY;\n   \n   -- Create policy for users to select only their own profile\n   CREATE POLICY profiles_select_policy ON profiles\n     FOR SELECT USING (auth.uid() = id);\n   \n   -- Create policy for users to update only their own profile\n   CREATE POLICY profiles_update_policy ON profiles\n     FOR UPDATE USING (auth.uid() = id);\n   ```\n\n2. Configure RLS policies for the 'interactions' table:\n   ```sql\n   -- Enable RLS on interactions table\n   ALTER TABLE interactions ENABLE ROW LEVEL SECURITY;\n   \n   -- Create policy for users to select only their own interactions\n   CREATE POLICY interactions_select_policy ON interactions\n     FOR SELECT USING (auth.uid() = user_id);\n   \n   -- Create policy for users to insert only their own interactions\n   CREATE POLICY interactions_insert_policy ON interactions\n     FOR INSERT WITH CHECK (auth.uid() = user_id);\n   \n   -- Create policy for users to update only their own interactions\n   CREATE POLICY interactions_update_policy ON interactions\n     FOR UPDATE USING (auth.uid() = user_id);\n   \n   -- Create policy for users to delete only their own interactions\n   CREATE POLICY interactions_delete_policy ON interactions\n     FOR DELETE USING (auth.uid() = user_id);\n   ```\n\n3. Create a special admin policy (if needed):\n   ```sql\n   -- Create policy for admins to access all profiles\n   CREATE POLICY profiles_admin_policy ON profiles\n     FOR ALL TO authenticated USING (\n       EXISTS (\n         SELECT 1 FROM profiles\n         WHERE id = auth.uid() AND is_admin = true\n       )\n     );\n   \n   -- Create policy for admins to access all interactions\n   CREATE POLICY interactions_admin_policy ON interactions\n     FOR ALL TO authenticated USING (\n       EXISTS (\n         SELECT 1 FROM profiles\n         WHERE id = auth.uid() AND is_admin = true\n       )\n     );\n   ```\n\n4. Verify RLS policies in Supabase dashboard:\n   - Navigate to Authentication → Policies\n   - Confirm all policies are correctly applied to tables\n   - Check policy definitions match intended access controls\n\n5. Document the RLS implementation for future reference, including:\n   - Policy names and purposes\n   - Access patterns (who can do what)\n   - Special considerations for admin users\n   - Any exceptions or edge cases",
        "testStrategy": "1. Create test users with different roles:\n   - Regular user A\n   - Regular user B\n   - Admin user (if applicable)\n\n2. Test 'profiles' table RLS:\n   - Authenticate as user A and verify they can only view/edit their own profile\n   - Authenticate as user A and attempt to access user B's profile (should be denied)\n   - Authenticate as user A and attempt to modify user B's profile (should be denied)\n   - If admin role exists, verify admin can access all profiles\n\n3. Test 'interactions' table RLS:\n   - Authenticate as user A and create new interactions\n   - Verify user A can view only their own interactions\n   - Authenticate as user B and verify they cannot see user A's interactions\n   - Authenticate as user A and attempt to modify user B's interactions (should be denied)\n   - If admin role exists, verify admin can access all interactions\n\n4. Test edge cases:\n   - Unauthenticated access attempts (should be denied)\n   - Batch operations affecting multiple users' data\n   - API access via different authentication methods\n\n5. Create automated tests using Supabase client:\n   ```javascript\n   // Example test for profiles RLS\n   const { data: ownProfile, error: ownError } = await supabase\n     .from('profiles')\n     .select('*')\n     .eq('id', user.id);\n   \n   // This should return empty array or error\n   const { data: otherProfile, error: otherError } = await supabase\n     .from('profiles')\n     .select('*')\n     .neq('id', user.id);\n   \n   // Assert that ownProfile contains data and otherProfile is empty\n   ```\n\n6. Document test results and any policy adjustments made during testing",
        "status": "pending",
        "dependencies": [
          1
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 18,
        "title": "Add Multiple Test Users to Supabase for RLS Policy Validation",
        "description": "Create and configure multiple test user accounts in the Supabase project to validate Row Level Security (RLS) policies and test user permission boundaries.",
        "details": "1. Create test users with different roles and permissions:\n   ```sql\n   -- Using Supabase Dashboard or SQL\n   -- Create regular test users\n   INSERT INTO auth.users (email, encrypted_password, email_confirmed_at, raw_user_meta_data)\n   VALUES \n     ('testuser1@example.com', crypt('securepassword1', gen_salt('bf')), now(), '{\"name\":\"Test User 1\",\"role\":\"regular\"}'),\n     ('testuser2@example.com', crypt('securepassword2', gen_salt('bf')), now(), '{\"name\":\"Test User 2\",\"role\":\"regular\"}'),\n     ('adminuser@example.com', crypt('adminpassword', gen_salt('bf')), now(), '{\"name\":\"Admin User\",\"role\":\"admin\"}');\n   \n   -- Create corresponding profile entries\n   INSERT INTO profiles (id, updated_at, interest_vector, interests)\n   VALUES \n     ([user1_id], now(), NULL, '{\"topics\":[\"technology\",\"science\"]}'),\n     ([user2_id], now(), NULL, '{\"topics\":[\"art\",\"history\"]}'),\n     ([admin_id], now(), NULL, '{\"topics\":[\"administration\",\"management\"]}');\n   ```\n\n2. Create a test script to validate RLS policies:\n   ```javascript\n   // src/tests/rls-validation.js\n   import { supabase } from '$lib/supabaseClient';\n   \n   async function testRLSPolicies() {\n     // Test user credentials\n     const users = [\n       { email: 'testuser1@example.com', password: 'securepassword1' },\n       { email: 'testuser2@example.com', password: 'securepassword2' },\n       { email: 'adminuser@example.com', password: 'adminpassword' }\n     ];\n     \n     const results = {};\n     \n     for (const user of users) {\n       // Login as user\n       const { data: authData, error: authError } = await supabase.auth.signInWithPassword({\n         email: user.email,\n         password: user.password\n       });\n       \n       if (authError) {\n         console.error(`Failed to login as ${user.email}:`, authError);\n         continue;\n       }\n       \n       // Test profiles table access\n       const { data: ownProfile, error: ownProfileError } = await supabase\n         .from('profiles')\n         .select('*')\n         .eq('id', authData.user.id)\n         .single();\n       \n       const { data: otherProfiles, error: otherProfilesError } = await supabase\n         .from('profiles')\n         .select('*')\n         .neq('id', authData.user.id);\n       \n       // Test interactions table access\n       const { data: ownInteractions, error: ownInteractionsError } = await supabase\n         .from('interactions')\n         .select('*')\n         .eq('user_id', authData.user.id);\n       \n       const { data: otherInteractions, error: otherInteractionsError } = await supabase\n         .from('interactions')\n         .select('*')\n         .neq('user_id', authData.user.id);\n       \n       // Store results\n       results[user.email] = {\n         ownProfile: { success: !ownProfileError, data: ownProfile, error: ownProfileError },\n         otherProfiles: { success: !otherProfilesError, data: otherProfiles, error: otherProfilesError },\n         ownInteractions: { success: !ownInteractionsError, data: ownInteractions, error: ownInteractionsError },\n         otherInteractions: { success: !otherInteractionsError, data: otherInteractions, error: otherInteractionsError }\n       };\n       \n       // Logout\n       await supabase.auth.signOut();\n     }\n     \n     return results;\n   }\n   ```\n\n3. Create a UI for managing test users (optional):\n   ```svelte\n   <!-- src/routes/admin/test-users/+page.svelte -->\n   <script>\n     import { onMount } from 'svelte';\n     import { supabase } from '$lib/supabaseClient';\n     \n     let testUsers = [];\n     let loading = true;\n     let newUser = { email: '', password: '', role: 'regular' };\n     \n     onMount(async () => {\n       await fetchTestUsers();\n     });\n     \n     async function fetchTestUsers() {\n       loading = true;\n       const { data, error } = await supabase\n         .from('auth.users')\n         .select('*')\n         .order('created_at', { ascending: false });\n         \n       if (error) {\n         console.error('Error fetching test users:', error);\n       } else {\n         testUsers = data;\n       }\n       loading = false;\n     }\n     \n     async function createTestUser() {\n       // Implementation for creating a new test user\n     }\n   </script>\n   \n   <div class=\"container mx-auto p-4\">\n     <h1 class=\"text-2xl font-bold mb-4\">Test User Management</h1>\n     \n     <!-- Form for creating new test users -->\n     <!-- List of existing test users -->\n   </div>\n   ```\n\n4. Document test user credentials in a secure location for team reference:\n   - Create a secure document with test user credentials\n   - Store in a password manager or secure team documentation\n   - Include user roles, permissions, and test scenarios for each user",
        "testStrategy": "1. Verify test user creation:\n   - Confirm all test users are properly created in the Supabase auth system\n   - Verify corresponding profile entries exist for each test user\n   - Check that user metadata (roles, etc.) is correctly stored\n\n2. Test RLS policies with each user type:\n   - Login as each test user through the application UI\n   - Verify users can only view and edit their own profiles\n   - Confirm users can only see their own interactions\n   - Test that users cannot modify other users' data\n   - If admin roles exist, verify appropriate elevated permissions\n\n3. Run the RLS validation script:\n   - Execute the test script and analyze results\n   - Verify expected access patterns are enforced\n   - Document any policy violations or unexpected behaviors\n\n4. Test boundary conditions:\n   - Attempt to access data at the edges of permission boundaries\n   - Test with users who have no data yet\n   - Verify behavior when a user is deleted or deactivated\n\n5. Document test results:\n   - Create a comprehensive report of RLS policy effectiveness\n   - Document any vulnerabilities or issues discovered\n   - Provide recommendations for policy improvements if needed",
        "status": "pending",
        "dependencies": [
          1,
          17
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Incorporate Data Engineering Sources into Content Collection Workflows",
        "description": "Extend the existing n8n workflows to include additional data engineering sources such as blogs, YouTube channels, and community forums for content collection and processing.",
        "details": "1. Update the existing n8n content collection workflow to include new data engineering sources:\n   \n   a. Add HTTP Request nodes for blog sources:\n   ```javascript\n   // Example configuration for blog RSS feeds\n   const blogSources = [\n     { name: 'Towards Data Science', url: 'https://towardsdatascience.com/feed' },\n     { name: 'Data Engineering Labs', url: 'https://dataengineeringlabs.com/feed' },\n     { name: 'Analytics Vidhya', url: 'https://medium.com/feed/analytics-vidhya' }\n   ];\n   \n   // Process each blog source\n   return blogSources.map(source => ({\n     json: {\n       name: source.name,\n       url: source.url,\n       type: 'blog'\n     }\n   }));\n   ```\n   \n   b. Extend YouTube collection to include data engineering channels:\n   ```javascript\n   // Data Engineering YouTube channels to monitor\n   const dataEngineeringChannels = [\n     'UCPyoJR47kPUW2UT6Qkx6k1w', // Data Engineering with Alexey\n     'UC3RKA4vunFAfrfxiJhPEplw', // Seattle Data Guy\n     'UCW8Ews7tdKKkBT6GdtQaXvQ', // Karolina Sowinska\n     'UC-QDfvrRIDB6F0bIO4I4HkQ'  // Andreas Kretz\n   ];\n   \n   // Add to existing YouTube collection\n   ```\n   \n   c. Add HTTP Request nodes for community forums:\n   ```javascript\n   // Example configuration for data engineering communities\n   const communityForums = [\n     { name: 'Data Engineering Reddit', url: 'https://www.reddit.com/r/dataengineering/new.json?limit=25' },\n     { name: 'Hacker News API', url: 'https://hn.algolia.com/api/v1/search_by_date?query=data%20engineering&tags=story' }\n   ];\n   ```\n\n2. Create a Function node to normalize data from different sources:\n   ```javascript\n   // Normalize content from different sources\n   function normalizeContent(items, source) {\n     switch(source) {\n       case 'blog':\n         return {\n           title: items.title,\n           url: items.link,\n           source: items.source.name,\n           full_text: items.content,\n           metadata: {\n             author: items.author,\n             published_date: items.pubDate,\n             categories: items.categories\n           }\n         };\n       case 'youtube':\n         return {\n           title: items.snippet.title,\n           url: `https://www.youtube.com/watch?v=${items.id.videoId}`,\n           source: 'YouTube',\n           full_text: '', // Will be populated by transcript in another step\n           metadata: {\n             channel: items.snippet.channelTitle,\n             published_date: items.snippet.publishedAt,\n             thumbnail: items.snippet.thumbnails.high.url\n           }\n         };\n       case 'community':\n         // Handle community-specific format\n         // ...\n     }\n   }\n   ```\n\n3. Modify the YouTube transcript workflow (Task 9) to handle the expanded list of channels:\n   ```javascript\n   // Ensure transcript fetching can handle increased volume\n   // Add rate limiting and error handling\n   const { YoutubeTranscript } = require('youtube-transcript');\n   \n   async function getTranscript(videoId) {\n     try {\n       // Add delay between requests to avoid rate limiting\n       await new Promise(resolve => setTimeout(resolve, 500));\n       \n       const transcript = await YoutubeTranscript.fetchTranscript(videoId);\n       return transcript.map(item => item.text).join(' ');\n     } catch (error) {\n       console.error(`Failed to get transcript for ${videoId}: ${error.message}`);\n       return ''; // Return empty string if transcript unavailable\n     }\n   }\n   ```\n\n4. Update the content filtering logic to handle domain-specific relevance:\n   ```javascript\n   // Filter function to ensure content is relevant to data engineering\n   function isDataEngineeringRelevant(content) {\n     const keywords = [\n       'data engineering', 'ETL', 'data pipeline', 'data warehouse',\n       'data lake', 'Spark', 'Kafka', 'Airflow', 'dbt', 'Snowflake',\n       'BigQuery', 'Redshift', 'data modeling', 'data governance'\n     ];\n     \n     const contentText = (content.title + ' ' + content.full_text).toLowerCase();\n     return keywords.some(keyword => contentText.includes(keyword.toLowerCase()));\n   }\n   ```\n\n5. Configure proper error handling and logging for new sources:\n   ```javascript\n   // Add comprehensive error handling\n   try {\n     // Source fetching code\n   } catch (error) {\n     console.error(`Error fetching from ${source.name}: ${error.message}`);\n     \n     // Log to monitoring system\n     await sendToMonitoring({\n       source: source.name,\n       error: error.message,\n       timestamp: new Date().toISOString()\n     });\n     \n     // Continue with other sources\n     return [];\n   }\n   ```\n\n6. Update the Supabase storage logic to include source-specific metadata:\n   ```javascript\n   // Enhanced storage with source categorization\n   const { data, error } = await supabase\n     .from('content')\n     .insert({\n       source: item.source,\n       url: item.url,\n       title: item.title,\n       full_text: item.full_text,\n       metadata: {\n         ...item.metadata,\n         source_category: determineSourceCategory(item.source),\n         content_type: determineContentType(item.url)\n       },\n       embedding: null // Will be populated by the embedding workflow\n     });\n   ```",
        "testStrategy": "1. Test the integration of each new data source individually:\n   - Verify that blog RSS feeds are correctly fetched and parsed\n   - Confirm YouTube data engineering channels are properly monitored\n   - Ensure community forums data is retrieved and normalized correctly\n\n2. Test the content normalization function:\n   - Create test cases for each source type (blog, YouTube, community)\n   - Verify that all required fields are correctly populated\n   - Check that metadata is properly structured for each source\n\n3. Test the YouTube transcript functionality with the expanded channel list:\n   - Verify transcripts are fetched for videos from new data engineering channels\n   - Test handling of videos without available transcripts\n   - Measure performance with the increased volume of requests\n\n4. Test the data engineering relevance filtering:\n   - Create a test set of relevant and non-relevant content\n   - Verify that the filtering correctly identifies data engineering content\n   - Adjust keywords if necessary to improve precision and recall\n\n5. Test error handling and resilience:\n   - Simulate network failures for each source type\n   - Verify that errors for one source don't prevent processing of other sources\n   - Check that appropriate error logs are generated\n\n6. End-to-end workflow testing:\n   - Run the complete workflow with all sources enabled\n   - Verify that content is correctly collected, processed, and stored in Supabase\n   - Check for duplicates and ensure deduplication is working\n\n7. Performance testing:\n   - Measure execution time with the expanded source list\n   - Identify and optimize any bottlenecks\n   - Ensure the workflow completes within reasonable time constraints",
        "status": "pending",
        "dependencies": [
          4,
          9
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 20,
        "title": "Create HNSW Vector Index for Content Embeddings",
        "description": "Implement a specialized HNSW (Hierarchical Navigable Small World) vector index on the embedding column of the content table to enable efficient semantic similarity searches and improve content ranking performance.",
        "details": "1. Analyze current table statistics and embedding dimensions:\n   ```sql\n   SELECT COUNT(*), pg_size_pretty(pg_total_relation_size('content')) FROM content;\n   ```\n\n2. Create the HNSW index with optimized parameters:\n   ```sql\n   -- Create HNSW index with recommended parameters for semantic search\n   CREATE INDEX hnsw_content_embedding_idx ON content \n   USING hnsw (embedding vector_l2_ops)\n   WITH (\n     m = 16,        -- max number of connections per layer\n     ef_construction = 64  -- size of dynamic candidate list for construction\n   );\n   ```\n\n3. Configure index parameters in postgresql.conf:\n   ```\n   maintenance_work_mem = '2GB'  -- Adjust based on available resources\n   effective_cache_size = '4GB'  -- Tune for better query planning\n   ```\n\n4. Implement index usage monitoring:\n   ```sql\n   CREATE OR REPLACE FUNCTION log_vector_search_stats()\n   RETURNS trigger AS $$\n   BEGIN\n     INSERT INTO search_stats (query_time, scan_type, rows_fetched)\n     VALUES (clock_timestamp() - query_start, current_setting('enable_seqscan'), rows_fetched);\n     RETURN NULL;\n   END;\n   $$ LANGUAGE plpgsql;\n   ```\n\n5. Update existing vector search queries to utilize the index:\n   ```sql\n   -- Example optimized query\n   SELECT id, title, 1 - (embedding <=> query_vector) as similarity\n   FROM content\n   ORDER BY embedding <=> query_vector\n   LIMIT 10;\n   ```",
        "testStrategy": "1. Benchmark index creation time and resource usage:\n   - Monitor CPU and memory consumption during index creation\n   - Record index size and creation duration\n\n2. Performance testing:\n   - Compare query execution times before and after index creation\n   - Test with varying dataset sizes (1K, 10K, 100K vectors)\n   - Measure query latency under concurrent load (10, 50, 100 simultaneous queries)\n\n3. Quality validation:\n   - Verify search results accuracy matches non-indexed queries\n   - Compare recall rates for different ef_search values\n   - Test with diverse query vectors\n\n4. Index maintenance testing:\n   - Verify index updates correctly with new content insertions\n   - Test index behavior during bulk updates\n   - Monitor index fragmentation over time\n\n5. Integration testing:\n   - Verify index usage in content ranking function\n   - Confirm proper index utilization in EXPLAIN ANALYZE output\n   - Test failover to sequential scan when appropriate",
        "status": "pending",
        "dependencies": [
          1,
          7,
          16
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 21,
        "title": "Clone and Set Up Memory Bank Repository",
        "description": "Clone the cursor-memory-bank repository and set up its basic configuration.",
        "details": "1. Clone the repository from https://github.com/vanzan01/cursor-memory-bank.git into a suitable directory within the project.\n2. Review the README.md to understand the basic file structure and setup instructions.\n3. Create a dedicated directory for memory bank files if not already present, e.g., `.memory_bank/`.",
        "testStrategy": "1. Verify that the repository is cloned successfully.\n2. Confirm that the directory structure matches the one described in the Memory Bank's documentation.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 22,
        "title": "Configure Custom Modes in Cursor for Memory Bank",
        "description": "Create the necessary context files (`bank.md`, `reflect.md`, `van.md`) in the `.cursor/context` directory to enable Memory Bank's custom operational modes.",
        "details": "1. Create a `.cursor/context` directory if it doesn't exist.\n2. Create `bank.md` file and populate it with the specified content for banking memories.\n3. Create `reflect.md` file for the self-reflection mode.\n4. Create `van.md` file for the Venture, Analysis, Narrative mode.\n5. Ensure the content of these files matches the instructions from the Memory Bank repository to correctly prime the AI.",
        "testStrategy": "1. In Cursor, try activating each mode (e.g., by mentioning '@van' or '@reflect').\n2. Verify that the AI's response and behavior align with the instructions defined in the corresponding markdown file.",
        "priority": "high",
        "dependencies": [
          21
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 23,
        "title": "Document Initial Project State with Memory Bank (VAN Mode)",
        "description": "Use the VAN (Venture, Analysis, Narrative) mode to conduct an initial analysis of the InsightHub project and create the first 'memory' file.",
        "details": "1. Activate VAN mode in Cursor.\n2. Perform a comprehensive analysis of the existing InsightHub project, including its goals, current architecture, key files, and completed tasks.\n3. Synthesize this analysis into a clear narrative.\n4. Save this initial analysis as the first memory file in the `.memory_bank/` directory (e.g., `001_initial_project_state.md`).",
        "testStrategy": "1. Review the generated memory file for accuracy and completeness.\n2. In a new session, use the 'bank' mode to load this memory and ask the AI a question about the project to see if it can answer correctly based on the banked knowledge.",
        "priority": "medium",
        "dependencies": [
          22
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 24,
        "title": "Define Integration Strategy between Taskmaster and Memory Bank",
        "description": "Analyze and document how Taskmaster's task management will complement Memory Bank's context management. Define a clear, combined workflow.",
        "details": "1. Document the distinct roles: Taskmaster for 'what' (tasks) and Memory Bank for 'why' and 'how' (context, decisions).\n2. Create a procedural document (e.g., `docs/DEVELOPMENT_WORKFLOW.md`) outlining the new integrated process.\n3. The workflow should cover: starting a new feature, daily progress, handling roadblocks, and completing a feature.\n4. Example flow: Use VAN mode to scope a new feature -> Create tasks in Taskmaster -> Use IMPLEMENT mode for coding -> Use REFLECT mode to document learnings -> Bank the reflection in Memory Bank -> Update task status in Taskmaster.",
        "testStrategy": "1. Peer-review the documented workflow for clarity and feasibility.\n2. Manually walk through one or two existing tasks using the new documented workflow to identify any gaps or friction points.",
        "priority": "medium",
        "dependencies": [
          23
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 25,
        "title": "Pilot a Feature with the Integrated Workflow",
        "description": "Select a small, low-risk feature or refactoring task and implement it end-to-end using the newly defined Taskmaster + Memory Bank workflow.",
        "details": "1. Choose a suitable pending task from the Taskmaster list (e.g., a small bug fix or a minor UI improvement).\n2. Follow the process documented in `docs/DEVELOPMENT_WORKFLOW.md` precisely.\n3. Create all associated artifacts: VAN analysis, implementation logs, reflection notes, and banked memories.\n4. Pay close attention to the friction and benefits of using the two systems together.",
        "testStrategy": "1. Verify that the feature is implemented correctly.\n2. Review all generated Memory Bank artifacts for quality and usefulness.\n3. Conduct a final reflection on the pilot process itself to identify areas for improvement in the integrated workflow.",
        "priority": "medium",
        "dependencies": [
          24
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 26,
        "title": "Integrate Gemini CLI as Auxiliary AI Tool",
        "description": "Install and configure Gemini CLI as an auxiliary AI tool for research, code review, and automation workflows, with full documentation and CI integration.",
        "details": "1. **Installation and Setup**:\n   - Install Gemini CLI globally: `npm install -g @google/generative-ai-cli` or use appropriate package manager\n   - Verify installation: `gemini --version`\n   - Set up configuration directory: `~/.config/gemini/`\n\n2. **Authentication Configuration**:\n   - Obtain Google AI Studio API key from https://makersuite.google.com/app/apikey\n   - Configure authentication: `gemini auth login` or set environment variable `GEMINI_API_KEY`\n   - Test authentication: `gemini models list`\n\n3. **Project Script Setup**:\n   - Create `scripts/gemini/` directory in project root\n   - Implement wrapper scripts for common tasks:\n     ```bash\n     # scripts/gemini/code-review.sh\n     #!/bin/bash\n     gemini generate --model=gemini-pro --prompt=\"Review this code for best practices, security issues, and improvements: $(cat $1)\"\n     \n     # scripts/gemini/research.sh\n     #!/bin/bash\n     gemini generate --model=gemini-pro --prompt=\"Research and summarize: $1\"\n     ```\n   - Create configuration file `scripts/gemini/config.json` with project-specific prompts and settings\n   - Add npm scripts to package.json for easy access\n\n4. **Documentation**:\n   - Create `docs/GEMINI_INTEGRATION.md` with:\n     - Installation instructions\n     - Authentication setup\n     - Available commands and use cases\n     - Integration with existing workflows\n     - Best practices and limitations\n   - Update main README.md with Gemini CLI section\n   - Document prompt templates and examples\n\n5. **CI Integration**:\n   - Add Gemini CLI to CI/CD pipeline dependencies\n   - Create GitHub Actions workflow for automated code review using Gemini\n   - Set up environment variables in CI for authentication\n   - Implement automated research tasks for documentation updates\n   - Configure rate limiting and error handling for CI usage\n\n6. **Workflow Integration**:\n   - Integrate with existing n8n workflows for content analysis\n   - Create Gemini nodes for automated content evaluation\n   - Set up fallback mechanisms when Gemini API is unavailable",
        "testStrategy": "1. **Installation Testing**:\n   - Verify Gemini CLI installs correctly on different environments (local, CI, Docker)\n   - Test version compatibility and dependency resolution\n   - Confirm CLI commands are accessible from project scripts\n\n2. **Authentication Testing**:\n   - Test API key authentication in multiple environments\n   - Verify authentication persists across sessions\n   - Test authentication failure handling and error messages\n\n3. **Functionality Testing**:\n   - Test code review functionality with sample code files\n   - Verify research capabilities with various query types\n   - Test prompt template system with different inputs\n   - Validate output formatting and parsing\n\n4. **Integration Testing**:\n   - Test npm script integration and execution\n   - Verify CI/CD pipeline integration without breaking existing workflows\n   - Test n8n workflow integration with Gemini nodes\n   - Validate rate limiting and error handling in automated scenarios\n\n5. **Documentation Testing**:\n   - Verify all documented commands work as described\n   - Test setup instructions on clean environment\n   - Validate example prompts and expected outputs\n   - Ensure troubleshooting guide covers common issues\n\n6. **Performance Testing**:\n   - Measure API response times for different query types\n   - Test concurrent usage limits and rate limiting\n   - Validate memory and resource usage during extended operations",
        "status": "pending",
        "dependencies": [
          2,
          5
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Install and Configure Gemini CLI",
            "description": "Install Gemini CLI globally and set up basic configuration structure for the project",
            "dependencies": [],
            "details": "Install Gemini CLI using npm: `npm install -g @google/generative-ai-cli`. Verify installation with `gemini --version`. Create configuration directory structure: `~/.config/gemini/` for global config and `scripts/gemini/` in project root. Set up basic project structure for Gemini integration including placeholder config files.",
            "status": "done",
            "testStrategy": "Verify installation by running `gemini --version` and confirm directory structure is created correctly"
          },
          {
            "id": 2,
            "title": "Set Up Authentication and API Access",
            "description": "Configure authentication for Gemini CLI using Google AI Studio API key and test connectivity",
            "dependencies": [
              1
            ],
            "details": "Obtain API key from Google AI Studio (https://makersuite.google.com/app/apikey). Configure authentication using `gemini auth login` or set `GEMINI_API_KEY` environment variable. Add API key to project's .env file with appropriate variable name. Test authentication by running `gemini models list` to verify API access and available models.",
            "status": "in-progress",
            "testStrategy": "Run `gemini models list` to confirm authentication works and API connectivity is established"
          },
          {
            "id": 3,
            "title": "Create Project Scripts and Wrapper Functions",
            "description": "Implement wrapper scripts for common Gemini tasks including code review, research, and automation workflows",
            "dependencies": [
              2
            ],
            "details": "Create `scripts/gemini/code-review.sh` for automated code review, `scripts/gemini/research.sh` for research tasks, and `scripts/gemini/config.json` for project-specific prompts and settings. Implement wrapper functions that handle common use cases like file analysis, prompt templating, and output formatting. Add npm scripts to package.json for easy access: `npm run gemini:review`, `npm run gemini:research`.",
            "status": "pending",
            "testStrategy": "Test each wrapper script with sample inputs and verify outputs are properly formatted and functional"
          },
          {
            "id": 4,
            "title": "Create Comprehensive Usage Documentation",
            "description": "Document Gemini CLI integration with installation instructions, usage examples, and best practices",
            "dependencies": [
              3
            ],
            "details": "Create `docs/GEMINI_INTEGRATION.md` with complete installation guide, authentication setup, available commands, use cases, and workflow integration examples. Update main README.md with Gemini CLI section. Document prompt templates, rate limiting considerations, error handling, and troubleshooting guide. Include examples of common tasks and integration patterns with existing project workflows.",
            "status": "pending",
            "testStrategy": "Review documentation for completeness and test all documented commands and examples to ensure accuracy"
          },
          {
            "id": 5,
            "title": "Integrate with Taskmaster and CI Pipeline",
            "description": "Set up Gemini CLI integration with Taskmaster system and CI/CD pipeline for automated workflows",
            "dependencies": [
              4
            ],
            "details": "Add Gemini CLI to CI/CD pipeline dependencies and configure environment variables for authentication. Create GitHub Actions workflow for automated code review using Gemini. Integrate with Taskmaster by adding Gemini commands to task execution options and creating task templates that utilize Gemini for research and analysis. Implement rate limiting, error handling, and fallback mechanisms for CI usage. Set up automated documentation updates using Gemini research capabilities.",
            "status": "pending",
            "testStrategy": "Test CI integration by triggering automated workflows and verify Taskmaster integration by running tasks that utilize Gemini functionality"
          }
        ]
      },
      {
        "id": 27,
        "title": "Archive n8n Infrastructure and Transition to LangChain Development",
        "description": "Archive all n8n-related files and retire the n8n docker-compose server, moving workflows and configurations to an 'archived_n8n' folder to prepare for LangChain-based development.",
        "details": "1. **Create Archive Structure**:\n   - Create `archived_n8n/` directory in project root\n   - Create subdirectories: `workflows/`, `configs/`, `docker/`, `docs/`\n\n2. **Archive n8n Workflows**:\n   - Export all existing n8n workflows from the n8n interface (JSON format)\n   - Save workflow exports to `archived_n8n/workflows/`\n   - Document workflow purposes and dependencies in `archived_n8n/workflows/README.md`\n\n3. **Archive Configuration Files**:\n   - Move n8n environment files (.env files) to `archived_n8n/configs/`\n   - Archive docker-compose.yml and related Docker configurations to `archived_n8n/docker/`\n   - Save any custom n8n node configurations or credentials templates\n\n4. **Document Migration Context**:\n   - Create `archived_n8n/MIGRATION_NOTES.md` documenting:\n     - Reasons for transition from n8n to LangChain\n     - Key workflows that need to be reimplemented\n     - Data sources and integrations to preserve\n     - Timeline and migration strategy\n\n5. **Clean Up Active Environment**:\n   - Stop and remove n8n docker containers: `docker-compose down`\n   - Remove n8n-related entries from main docker-compose.yml\n   - Update .gitignore to exclude archived files from active development\n   - Remove n8n-related environment variables from active .env files\n\n6. **Prepare LangChain Foundation**:\n   - Update project README.md to reflect the transition\n   - Create placeholder directories for LangChain implementation\n   - Update development documentation to remove n8n references",
        "testStrategy": "1. **Archive Verification**:\n   - Verify all n8n workflows are successfully exported and saved\n   - Confirm all configuration files are properly archived\n   - Check that archived directory structure is complete and organized\n\n2. **Environment Cleanup Testing**:\n   - Verify n8n containers are completely stopped and removed\n   - Confirm no n8n processes are running: `docker ps | grep n8n`\n   - Test that main application still functions without n8n dependencies\n\n3. **Documentation Review**:\n   - Review migration notes for completeness and accuracy\n   - Verify that all critical workflow logic is documented for future LangChain implementation\n   - Confirm project README accurately reflects the new direction\n\n4. **Git Repository State**:\n   - Verify archived files are properly committed to version control\n   - Confirm .gitignore updates are working correctly\n   - Test that the repository is clean and ready for LangChain development",
        "status": "pending",
        "dependencies": [
          4
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 28,
        "title": "Setup LangChain Environment",
        "description": "Configure the Python project to support LangChain development. This includes adding all necessary dependencies to pyproject.toml and setting up a .env file for secure API key management.",
        "details": "1. **Update pyproject.toml with LangChain Dependencies**:\n   - Add core LangChain packages:\n     ```toml\n     [tool.poetry.dependencies]\n     langchain = \"^0.1.0\"\n     langchain-community = \"^0.0.10\"\n     langchain-openai = \"^0.0.5\"\n     langchain-google-genai = \"^0.0.6\"\n     python-dotenv = \"^1.0.0\"\n     ```\n   - Add optional dependencies for specific integrations:\n     ```toml\n     # Vector stores and embeddings\n     chromadb = \"^0.4.0\"\n     faiss-cpu = \"^1.7.4\"\n     sentence-transformers = \"^2.2.2\"\n     \n     # Document loaders\n     pypdf = \"^3.17.0\"\n     beautifulsoup4 = \"^4.12.0\"\n     requests = \"^2.31.0\"\n     ```\n\n2. **Create Environment Configuration**:\n   - Create `.env.example` file with template variables:\n     ```\n     # OpenAI Configuration\n     OPENAI_API_KEY=your_openai_api_key_here\n     \n     # Google AI Configuration\n     GOOGLE_API_KEY=your_google_api_key_here\n     \n     # Supabase Configuration (if integrating with existing DB)\n     SUPABASE_URL=your_supabase_url\n     SUPABASE_KEY=your_supabase_anon_key\n     \n     # LangChain Configuration\n     LANGCHAIN_TRACING_V2=true\n     LANGCHAIN_API_KEY=your_langsmith_api_key_here\n     ```\n   - Create actual `.env` file (ensure it's in .gitignore)\n   - Add `.env` to .gitignore if not already present\n\n3. **Install and Verify Dependencies**:\n   - Run `poetry install` to install all dependencies\n   - Create a simple verification script `scripts/verify_langchain.py`:\n     ```python\n     import os\n     from dotenv import load_dotenv\n     from langchain.llms import OpenAI\n     from langchain.schema import HumanMessage\n     \n     load_dotenv()\n     \n     def verify_setup():\n         print(\"Verifying LangChain environment setup...\")\n         \n         # Check environment variables\n         required_vars = ['OPENAI_API_KEY']\n         for var in required_vars:\n             if not os.getenv(var):\n                 print(f\"Warning: {var} not set\")\n         \n         # Test basic LangChain functionality\n         try:\n             llm = OpenAI(temperature=0)\n             response = llm(\"Hello, this is a test.\")\n             print(\"✓ LangChain basic functionality working\")\n         except Exception as e:\n             print(f\"✗ LangChain test failed: {e}\")\n     \n     if __name__ == \"__main__\":\n         verify_setup()\n     ```\n\n4. **Update Project Documentation**:\n   - Update README.md with LangChain setup instructions\n   - Document required API keys and how to obtain them\n   - Add examples of basic LangChain usage patterns",
        "testStrategy": "1. **Dependency Installation Testing**:\n   - Run `poetry install` and verify all LangChain packages install without conflicts\n   - Check that `poetry show langchain` displays the correct version\n   - Verify no dependency resolution errors or warnings\n\n2. **Environment Configuration Testing**:\n   - Verify `.env.example` contains all necessary template variables\n   - Test that `python-dotenv` can load environment variables correctly\n   - Confirm `.env` is properly ignored by git (check with `git status`)\n\n3. **Basic Functionality Testing**:\n   - Run the verification script: `python scripts/verify_langchain.py`\n   - Test basic LangChain imports in Python REPL:\n     ```python\n     from langchain.llms import OpenAI\n     from langchain.schema import HumanMessage\n     from langchain.chains import LLMChain\n     ```\n   - Verify API key authentication works (if keys are provided)\n\n4. **Integration Testing**:\n   - Test LangChain integration with existing Supabase setup (if applicable)\n   - Verify that LangChain can work alongside existing project dependencies\n   - Run existing project tests to ensure no regressions introduced",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 29,
        "title": "Implement YouTube Processing Pipeline",
        "description": "Create a Python script that takes a YouTube URL, loads the transcript using LangChain's YoutubeLoader, and generates a summary using an LLM. This script will serve as the first functional component of the new system.",
        "details": "1. **Create YouTube Processing Script**:\n   - Create `src/youtube_processor.py` with main processing class\n   - Implement `YouTubeProcessor` class with methods:\n     ```python\n     from langchain_community.document_loaders import YoutubeLoader\n     from langchain_openai import ChatOpenAI\n     from langchain.chains.summarize import load_summarize_chain\n     \n     class YouTubeProcessor:\n         def __init__(self, llm_model=\"gpt-3.5-turbo\"):\n             self.llm = ChatOpenAI(model=llm_model, temperature=0.3)\n             \n         def load_transcript(self, youtube_url):\n             loader = YoutubeLoader.from_youtube_url(youtube_url, add_video_info=True)\n             return loader.load()\n             \n         def generate_summary(self, documents):\n             chain = load_summarize_chain(self.llm, chain_type=\"stuff\")\n             return chain.run(documents)\n     ```\n\n2. **URL Validation and Error Handling**:\n   - Validate YouTube URL format using regex\n   - Handle cases where transcripts are unavailable\n   - Implement retry logic for API failures\n   - Add logging for debugging and monitoring\n\n3. **CLI Interface**:\n   - Create command-line interface using argparse\n   - Support options for different summary lengths and LLM models\n   - Output results to console and optionally save to file\n\n4. **Configuration Management**:\n   - Load API keys from .env file using python-dotenv\n   - Support configuration for different LLM providers (OpenAI, Google)\n   - Add configurable parameters for summary generation",
        "testStrategy": "1. **Unit Testing**:\n   - Test URL validation with valid and invalid YouTube URLs\n   - Mock YoutubeLoader to test transcript loading functionality\n   - Test error handling for videos without transcripts\n   - Verify LLM integration with mock responses\n\n2. **Integration Testing**:\n   - Test with real YouTube URLs that have transcripts available\n   - Verify end-to-end pipeline from URL input to summary output\n   - Test with different video lengths (short, medium, long)\n   - Validate summary quality and relevance\n\n3. **CLI Testing**:\n   - Test command-line interface with various argument combinations\n   - Verify help text and error messages are clear\n   - Test file output functionality\n\n4. **Environment Testing**:\n   - Verify script works with different API key configurations\n   - Test with different LLM models if multiple are supported\n   - Confirm proper error messages when API keys are missing",
        "status": "pending",
        "dependencies": [
          28
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 30,
        "title": "Implement Reddit Processing Pipeline",
        "description": "Develop a module to fetch content from specified Reddit subreddits and process it through the same summarization chain logic used for YouTube.",
        "details": "1. **Create Reddit Processing Script**:\n   - Create `src/reddit_processor.py` with main processing class\n   - Implement `RedditProcessor` class with methods:\n     ```python\n     import praw\n     from langchain_community.document_loaders import RedditPostsLoader\n     from langchain_openai import ChatOpenAI\n     from langchain.chains.summarize import load_summarize_chain\n     from langchain.text_splitter import RecursiveCharacterTextSplitter\n     \n     class RedditProcessor:\n         def __init__(self, llm_model=\"gpt-3.5-turbo\"):\n             self.llm = ChatOpenAI(model=llm_model, temperature=0.3)\n             self.reddit = praw.Reddit(\n                 client_id=os.getenv(\"REDDIT_CLIENT_ID\"),\n                 client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),\n                 user_agent=\"content-processor/1.0\"\n             )\n         \n         def fetch_subreddit_posts(self, subreddit_name, limit=10, time_filter='day'):\n             # Fetch posts from specified subreddit\n         \n         def process_post_content(self, post):\n             # Extract and clean post content including comments\n         \n         def generate_summary(self, content):\n             # Use same summarization chain as YouTube processor\n     ```\n\n2. **Implement Content Extraction**:\n   - Extract post title, selftext, and top comments\n   - Handle different post types (text, link, image posts)\n   - Implement content filtering and cleaning\n   - Add support for comment thread processing\n\n3. **Integrate with Existing Summarization Chain**:\n   - Reuse the summarization logic from YouTubeProcessor\n   - Create shared base class or utility functions for common processing\n   - Ensure consistent output format between YouTube and Reddit processors\n\n4. **Add Configuration and Error Handling**:\n   - Add Reddit API credentials to .env file\n   - Implement rate limiting and API quota management\n   - Handle private/deleted posts and subreddits\n   - Add logging and error recovery mechanisms\n\n5. **Create CLI Interface**:\n   - Add command-line interface for processing Reddit content\n   - Support batch processing of multiple subreddits\n   - Include options for filtering by post score, age, and content type",
        "testStrategy": "1. **Unit Testing**:\n   - Test subreddit name validation and sanitization\n   - Mock Reddit API responses to test content extraction\n   - Test error handling for private/banned subreddits\n   - Verify content filtering and cleaning functions\n   - Test summarization chain integration with Reddit content\n\n2. **Integration Testing**:\n   - Test with real Reddit API using test subreddits\n   - Verify rate limiting and API quota handling\n   - Test processing of different post types (text, link, image)\n   - Confirm consistent output format with YouTube processor\n   - Test batch processing of multiple subreddits\n\n3. **Performance Testing**:\n   - Measure processing time for different content volumes\n   - Test memory usage with large comment threads\n   - Verify API rate limit compliance\n   - Test concurrent processing capabilities\n\n4. **End-to-End Testing**:\n   - Process content from popular subreddits (r/technology, r/programming)\n   - Verify summary quality and consistency\n   - Test CLI interface with various parameter combinations\n   - Confirm integration with existing project structure",
        "status": "pending",
        "dependencies": [
          29
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 31,
        "title": "Design and Build Core Orchestrator with LangGraph",
        "description": "Create the main application orchestrator using LangGraph. Define the state machine, nodes, and edges required to manage the flow of content from both YouTube and Reddit through the processing pipelines.",
        "details": "1. **Install and Configure LangGraph**:\n   - Add LangGraph to pyproject.toml dependencies:\n     ```toml\n     [tool.poetry.dependencies]\n     langgraph = \"^0.0.40\"\n     langchain-core = \"^0.1.0\"\n     ```\n   - Create `src/orchestrator/` directory structure\n\n2. **Define Application State Schema**:\n   - Create `src/orchestrator/state.py` with TypedDict state definition:\n     ```python\n     from typing import TypedDict, List, Optional, Literal\n     from dataclasses import dataclass\n     \n     class ContentState(TypedDict):\n         source_type: Literal[\"youtube\", \"reddit\"]\n         source_url: str\n         raw_content: Optional[str]\n         processed_content: Optional[str]\n         summary: Optional[str]\n         embeddings: Optional[List[float]]\n         status: Literal[\"pending\", \"processing\", \"completed\", \"failed\"]\n         error_message: Optional[str]\n         metadata: dict\n     ```\n\n3. **Create Processing Nodes**:\n   - Implement `ContentFetcherNode` for routing to YouTube/Reddit processors\n   - Implement `SummarizerNode` for content summarization\n   - Implement `EmbeddingNode` for generating vector embeddings\n   - Implement `StorageNode` for database persistence\n   - Implement `ErrorHandlerNode` for failure recovery\n\n4. **Build State Graph**:\n   - Create `src/orchestrator/graph.py` with LangGraph StateGraph:\n     ```python\n     from langgraph.graph import StateGraph, END\n     from .nodes import ContentFetcherNode, SummarizerNode, EmbeddingNode, StorageNode\n     \n     def create_orchestrator_graph():\n         workflow = StateGraph(ContentState)\n         \n         # Add nodes\n         workflow.add_node(\"fetch\", ContentFetcherNode())\n         workflow.add_node(\"summarize\", SummarizerNode())\n         workflow.add_node(\"embed\", EmbeddingNode())\n         workflow.add_node(\"store\", StorageNode())\n         \n         # Define edges and conditional routing\n         workflow.add_edge(\"fetch\", \"summarize\")\n         workflow.add_edge(\"summarize\", \"embed\")\n         workflow.add_edge(\"embed\", \"store\")\n         workflow.add_edge(\"store\", END)\n         \n         workflow.set_entry_point(\"fetch\")\n         return workflow.compile()\n     ```\n\n5. **Implement Main Orchestrator Class**:\n   - Create `src/orchestrator/main.py` with orchestrator interface\n   - Add batch processing capabilities for multiple content items\n   - Implement progress tracking and status reporting\n   - Add configuration management for different processing modes\n\n6. **Error Handling and Recovery**:\n   - Implement retry logic for failed nodes\n   - Add circuit breaker patterns for external API calls\n   - Create comprehensive logging and monitoring hooks",
        "testStrategy": "1. **Unit Testing**:\n   - Test each node individually with mock inputs and verify state transitions\n   - Test state schema validation with valid and invalid state objects\n   - Verify error handling in each node with simulated failures\n   - Test conditional routing logic with different content types\n\n2. **Integration Testing**:\n   - Test complete workflow execution with sample YouTube and Reddit content\n   - Verify state persistence across node transitions\n   - Test parallel processing of multiple content items\n   - Validate integration with existing YouTube and Reddit processors\n\n3. **Performance Testing**:\n   - Benchmark orchestrator throughput with varying batch sizes\n   - Test memory usage and resource cleanup during long-running processes\n   - Measure latency for different workflow paths\n   - Test concurrent execution limits and resource contention\n\n4. **End-to-End Testing**:\n   - Process real YouTube videos and Reddit posts through complete pipeline\n   - Verify final content storage in database matches expected format\n   - Test error recovery scenarios with network failures and API timeouts\n   - Validate monitoring and logging output for operational visibility",
        "status": "pending",
        "dependencies": [
          30
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 32,
        "title": "Add Content Ranking and Scoring Logic",
        "description": "Integrate a new node into the LangGraph orchestrator that analyzes the processed content and assigns a relevance score based on predefined criteria.",
        "details": "1. **Create Content Scoring Node**:\n   - Create `src/orchestrator/nodes/content_scorer.py` with scoring logic:\n     ```python\n     from typing import Dict, Any, List\n     from langchain_core.runnables import RunnableLambda\n     from ..state import AppState\n     \n     class ContentScorer:\n         def __init__(self, scoring_weights: Dict[str, float] = None):\n             self.weights = scoring_weights or {\n                 \"content_quality\": 0.3,\n                 \"engagement_metrics\": 0.2,\n                 \"recency\": 0.15,\n                 \"source_credibility\": 0.2,\n                 \"content_length\": 0.15\n             }\n         \n         def score_content(self, state: AppState) -> AppState:\n             # Implement multi-criteria scoring logic\n             pass\n     ```\n\n2. **Implement Scoring Criteria**:\n   - Content quality score based on LLM analysis of summary coherence and informativeness\n   - Engagement metrics (YouTube views/likes, Reddit upvotes/comments)\n   - Recency factor with exponential decay for older content\n   - Source credibility scoring based on channel/subreddit reputation\n   - Content length optimization (penalize too short/long content)\n\n3. **Add Scoring Node to Orchestrator**:\n   - Integrate the scoring node into the LangGraph workflow after content processing\n   - Update state schema to include `relevance_score` field\n   - Add conditional routing based on score thresholds\n\n4. **Implement Score Normalization**:\n   - Normalize scores to 0-1 range for consistent ranking\n   - Apply user preference weighting if available\n   - Store scoring metadata for debugging and optimization",
        "testStrategy": "1. **Unit Testing**:\n   - Test each scoring criterion individually with mock content data\n   - Verify score normalization produces values in expected 0-1 range\n   - Test edge cases (missing metadata, zero engagement, etc.)\n   - Validate scoring weights sum to expected total\n\n2. **Integration Testing**:\n   - Test the scoring node within the complete LangGraph workflow\n   - Verify state transitions and score persistence through the pipeline\n   - Test with real YouTube and Reddit content samples\n   - Confirm scores are consistently applied across different content types\n\n3. **Performance Testing**:\n   - Measure scoring node execution time with varying content volumes\n   - Test memory usage during batch scoring operations\n   - Verify scoring doesn't become a bottleneck in the orchestrator flow\n\n4. **Quality Validation**:\n   - Manual review of scored content to validate ranking makes intuitive sense\n   - A/B test different scoring weight configurations\n   - Compare scores against human-rated content quality assessments",
        "status": "pending",
        "dependencies": [
          31
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 33,
        "title": "Implement Supabase Integration for Storage",
        "description": "Create a final node in the LangGraph orchestrator to connect to Supabase and store the summarized, ranked, and analyzed content in the database.",
        "details": "1. **Create Supabase Storage Node**:\n   - Create `src/orchestrator/nodes/supabase_storage.py` with database integration:\n     ```python\n     from typing import Dict, Any\n     from supabase import create_client, Client\n     from ..state import AppState\n     import os\n     from datetime import datetime\n     \n     class SupabaseStorageNode:\n         def __init__(self):\n             self.supabase: Client = create_client(\n                 os.getenv(\"SUPABASE_URL\"),\n                 os.getenv(\"SUPABASE_ANON_KEY\")\n             )\n         \n         async def store_content(self, state: AppState) -> AppState:\n             \"\"\"Store processed content in Supabase content table\"\"\"\n             try:\n                 content_data = {\n                     \"source\": state[\"content_source\"],\n                     \"url\": state[\"source_url\"],\n                     \"title\": state[\"processed_content\"][\"title\"],\n                     \"full_text\": state[\"processed_content\"][\"full_text\"],\n                     \"summary\": state[\"processed_content\"][\"summary\"],\n                     \"relevance_score\": state[\"content_score\"],\n                     \"metadata\": {\n                         \"processing_timestamp\": datetime.utcnow().isoformat(),\n                         \"content_type\": state[\"content_type\"],\n                         \"engagement_metrics\": state.get(\"engagement_metrics\", {}),\n                         \"scoring_breakdown\": state.get(\"scoring_details\", {})\n                     },\n                     \"embedding\": state[\"content_embedding\"]\n                 }\n                 \n                 result = self.supabase.table(\"content\").insert(content_data).execute()\n                 state[\"storage_result\"] = {\n                     \"success\": True,\n                     \"content_id\": result.data[0][\"id\"],\n                     \"stored_at\": datetime.utcnow().isoformat()\n                 }\n                 \n             except Exception as e:\n                 state[\"storage_result\"] = {\n                     \"success\": False,\n                     \"error\": str(e),\n                     \"attempted_at\": datetime.utcnow().isoformat()\n                 }\n             \n             return state\n     ```\n\n2. **Integrate Storage Node into LangGraph Workflow**:\n   - Update `src/orchestrator/graph.py` to include the storage node:\n     ```python\n     from .nodes.supabase_storage import SupabaseStorageNode\n     \n     # Add storage node to the graph\n     storage_node = SupabaseStorageNode()\n     graph.add_node(\"store_content\", storage_node.store_content)\n     \n     # Connect scoring node to storage node\n     graph.add_edge(\"score_content\", \"store_content\")\n     graph.add_edge(\"store_content\", END)\n     ```\n\n3. **Configure Environment Variables**:\n   - Add Supabase credentials to `.env`:\n     ```\n     SUPABASE_URL=your_supabase_project_url\n     SUPABASE_ANON_KEY=your_supabase_anon_key\n     SUPABASE_SERVICE_ROLE_KEY=your_service_role_key\n     ```\n\n4. **Add Error Handling and Retry Logic**:\n   - Implement exponential backoff for failed database operations\n   - Add validation for required fields before storage\n   - Handle embedding dimension mismatches\n   - Log storage operations for debugging\n\n5. **Update Dependencies**:\n   - Add supabase-py to pyproject.toml:\n     ```toml\n     [tool.poetry.dependencies]\n     supabase = \"^1.0.0\"\n     ```",
        "testStrategy": "1. **Unit Testing**:\n   - Mock Supabase client and test storage node with valid state data\n   - Test error handling with invalid/missing data fields\n   - Verify state updates after successful and failed storage operations\n   - Test embedding vector compatibility with database schema\n\n2. **Integration Testing**:\n   - Test end-to-end workflow from content processing to database storage\n   - Verify stored data integrity by querying the content table after storage\n   - Test with both YouTube and Reddit content types\n   - Validate metadata JSON structure and content\n\n3. **Database Testing**:\n   - Confirm content is stored in correct table with all required fields\n   - Test vector embedding storage and retrieval functionality\n   - Verify foreign key constraints and data relationships\n   - Test storage with different content sizes and embedding dimensions\n\n4. **Error Recovery Testing**:\n   - Test behavior with network connectivity issues\n   - Verify retry logic with temporary database unavailability\n   - Test handling of duplicate content storage attempts\n   - Validate error logging and state management during failures\n\n5. **Performance Testing**:\n   - Measure storage operation latency with different content sizes\n   - Test concurrent storage operations\n   - Monitor database connection pooling and resource usage",
        "status": "pending",
        "dependencies": [
          32
        ],
        "priority": "medium",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-06-22T04:32:04.974Z",
      "updated": "2025-06-27T06:46:02.504Z",
      "description": "Tasks for master context"
    }
  }
}